# Krampus Kart: PPO Training

A browser-based reinforcement learning demo where neural network-controlled cars learn to drive around a procedurally generated track using **Proximal Policy Optimization (PPO)**.

![Krampus Kart](https://img.shields.io/badge/RL-PPO-blue) ![TensorFlow.js](https://img.shields.io/badge/TensorFlow.js-2.x-orange) ![Vanilla JS](https://img.shields.io/badge/JS-Vanilla-yellow)

## ğŸ¯ PPO's Key Innovation: The Clipped Objective

PPO's main contribution ([Schulman et al., 2017](https://arxiv.org/abs/1707.06347)) is a simple way to prevent destructively large policy updates. The key equation is:

```
L^CLIP(Î¸) = E_t[ min( r_t(Î¸) Â· Ã‚_t,  clip(r_t(Î¸), 1-Îµ, 1+Îµ) Â· Ã‚_t ) ]
```

Where:
- **r_t(Î¸)** = Ï€_new(a|s) / Ï€_old(a|s) â€” how much the policy changed
- **Îµ** = 0.1 â€” the clip range (prevents ratio from going outside [0.9, 1.1])
- **Ã‚_t** = advantage â€” "was this action better than expected?"

**Why it matters:** Without clipping, policy gradient can make huge updates that break learning. PPO clips the objective so that even if the optimizer *wants* to make a big change, the gradient is zeroed out when the ratio strays too far from 1.

### ğŸ“ Where to find it in the code

**[`js/ppo/ppo-agent.js`](js/ppo/ppo-agent.js) lines 114-120:**
```javascript
const ratio = tf.exp(tf.sub(newLogProbs, oldLogProbs));  // r_t(Î¸)
const surr1 = tf.mul(ratio, advantages);                 // r_t Â· Ã‚_t
const clippedRatio = tf.clipByValue(ratio, 1 - Îµ, 1 + Îµ);// clip(r_t, 1-Îµ, 1+Îµ)
const surr2 = tf.mul(clippedRatio, advantages);          // clipped Â· Ã‚_t
const policyLoss = tf.neg(tf.mean(tf.minimum(surr1, surr2))); // min(...)
```

The clip epsilon (`Îµ = 0.1`) is set in **[`js/config.js`](js/config.js)** as `CLIP_EPSILON`.

---

## ğŸš€ Try It Live

**[https://mattloper.github.io/krampuskart-ppo/](https://mattloper.github.io/krampuskart-ppo/)**

Or run locally:
```bash
python3 -m http.server 8080
# Open http://localhost:8080
```

## What It Does

24 cars spawn at random positions near the start line and learn to drive through trial and error. The neural network receives sensor data and outputs steering commands (throttle is always forward). Cars are colored using a **jet colormap** based on their per-step reward (red = low, blue = high).

Over time, cars learn to maximize **forward progress** along the track.

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Browser                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  index.html                                                  â”‚
â”‚  â”œâ”€â”€ TensorFlow.js (CDN)                                    â”‚
â”‚  â”œâ”€â”€ styles.css                                             â”‚
â”‚  â””â”€â”€ js/                                                    â”‚
â”‚       â”œâ”€â”€ main.js          â† Game loop, PPO training        â”‚
â”‚       â”œâ”€â”€ car.js           â† Car physics, sensors, state    â”‚
â”‚       â”œâ”€â”€ track.js         â† Procedural track, SDF, progressâ”‚
â”‚       â”œâ”€â”€ spline.js        â† Catmull-Rom spline math        â”‚
â”‚       â”œâ”€â”€ config.js        â† All hyperparameters            â”‚
â”‚       â”œâ”€â”€ ui.js            â† HUD updates                    â”‚
â”‚       â”œâ”€â”€ utils.js         â† Helper functions               â”‚
â”‚       â”œâ”€â”€ charts.js        â† Avg reward chart               â”‚
â”‚       â”œâ”€â”€ nn-visualizer.js â† Neural network weight viz      â”‚
â”‚       â”œâ”€â”€ simulation.js    â† Car spawning, camera, helpers  â”‚
â”‚       â”œâ”€â”€ debug-logger.js  â† Step & update logging          â”‚
â”‚       â””â”€â”€ ppo/                                              â”‚
â”‚            â”œâ”€â”€ actor-critic.js    â† Neural network (TF.js)  â”‚
â”‚            â”œâ”€â”€ ppo-agent.js       â† PPO algorithm           â”‚
â”‚            â”œâ”€â”€ experience-buffer.js â† Rollout storage, GAE  â”‚
â”‚            â””â”€â”€ reward.js          â† Reward computation      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Neural Network

**Input (10 dimensions):**
- 8 LIDAR sensor distances (normalized 0-1, max range 600)
- Current speed (normalized)
- Signed angle to track direction (normalized to [-1, 1])

**Architecture (Separate Networks):**
- **Actor Network**: Input â†’ 4 hidden units (GELU) â†’ steering mean + learned log-std
- **Critic Network**: Input â†’ 4 hidden units (GELU) â†’ state value
- Networks are **separate** (no shared backbone), as recommended by the PPO paper for continuous control

See **[`js/ppo/actor-critic.js`](js/ppo/actor-critic.js)** for implementation.

**Output (1 continuous action):**
- Steering: relative turn rate [-1, 1] (added to current heading each frame)
- Throttle is hardcoded to always forward (1.0)

**Pretraining:**
Before PPO starts, the network is pretrained with behavioral cloning for 20 epochs to learn a simple policy: counter-steer proportional to angle error.

**Visualization:**
The neural network weights are visualized in the lower-right corner using a jet colormap (red = negative weights, blue = positive weights).

## Reward Function

```
reward = deltaProgress Ã— 500
```

The reward is simply proportional to forward progress along the track. Clearance, angle alignment, and death penalties are all disabled for simplicity.

## PPO Hyperparameters

| Parameter | Value | Description |
|-----------|-------|-------------|
| `GAMMA` | 0.995 | Discount factor (long horizon) |
| `GAE_LAMBDA` | 0.95 | Advantage estimation |
| `CLIP_EPSILON` | 0.1 | Policy clip range |
| `LEARNING_RATE` | 3e-4 | Adam optimizer LR |
| `ROLLOUT_LENGTH` | 1024 | Buffer size before update |
| `EPOCHS_PER_UPDATE` | 10 | PPO epochs per rollout |
| `BATCH_SIZE` | 64 | Minibatch size |
| `VALUE_COEF` | 10.0 | Critic loss weight |
| `ENTROPY_COEF` | 0.01 | Exploration bonus |

## Complete Episodes Training

Only **complete episodes** (cars that crash or finish a lap) are used for training. This provides clean Monte Carlo returns without needing to bootstrap incomplete trajectories.

## Critic Training

The critic is trained using **Monte Carlo returns** (actual observed discounted rewards) rather than TD(Î») bootstrapped targets. This prevents the circular dependency where bad value predictions create bad training targets.

## UI Indicators

| Indicator | Meaning |
|-----------|---------|
| **PPO Updates** | Number of policy updates |
| **Best Episode** | Highest episode reward seen |
| **Cars Active** | Cars currently driving (not crashed) |
| **Avg Reward** | Rolling average of recent episode rewards |
| **Leader Progress** | Best car's track progress (% of lap) |
| **Pred / Real / Err** | Critic prediction vs actual discounted return |
| **Progress Reward** | Average progress reward per episode |
| **Grads: âœ“/âœ—** | Whether gradients are flowing |

## Visualizations

**Lower Left - Avg Reward Chart:**
Shows average episode reward over time (green line).

**Lower Right - Neural Network:**
Shows network weights as colored connections using a jet colormap:
- ğŸ”´ Red = negative weights
- ğŸŸ¢ Green/Yellow = near-zero weights  
- ğŸ”µ Blue = positive weights

## Car Colors

Cars are colored using a **jet colormap** based on their **cumulative episode reward**:
- ğŸ”´ Red = just spawned (low accumulated reward)
- ğŸŸ¡ Yellow/Green = medium progress
- ğŸ”µ Blue = survived longest / most reward

The leader car (furthest ahead) has a white outline and visible LIDAR beams.

## Spawning & Collisions

**Spawning:**
Cars spawn at random positions near the start line (not in a fixed grid). Spawn positions are validated to be on the track.

**Episode Termination:**
Cars only die from:
- **Wall collision** - driving off the track
- **Car-car collision** - after 60-frame grace period

There is **no timeout** - cars can take as long as needed.

## How Learning Works

| Step | What happens | Code |
|------|--------------|------|
| 1. **Pretrain** | Behavioral cloning teaches basic steering | [`actor-critic.js:pretrain()`](js/ppo/actor-critic.js) |
| 2. **Rollout** | 24 cars drive, collecting (state, action, reward, value) | [`main.js:ppoStep()`](js/main.js) |
| 3. **Complete Episodes** | Only crashed/finished cars' data goes to buffer | [`main.js:resetFinishedCars()`](js/main.js) |
| 4. **Returns** | Compute Monte Carlo returns (actual discounted rewards) | [`experience-buffer.js`](js/ppo/experience-buffer.js) |
| 5. **Advantages** | Compute GAE advantages for policy gradient | [`experience-buffer.js`](js/ppo/experience-buffer.js) |
| 6. **PPO Update** | **The key part!** Clipped surrogate loss, 10 epochs | [`ppo-agent.js:_updateBatch()`](js/ppo/ppo-agent.js) |
| 7. **Repeat** | Clear buffer, continue collecting | [`ppo-agent.js:update()`](js/ppo/ppo-agent.js) |

## Dependencies

- **TensorFlow.js** (loaded from CDN)
- **Tailwind CSS** (loaded from CDN, dev only)
- Python 3 (for local server)

No npm, no build step. Just a browser and Python.

## License

MIT
